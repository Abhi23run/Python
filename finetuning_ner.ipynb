{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN/1kXMM1+BClR/JDp9nUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhi23run/Python/blob/main/finetuning_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMKZlUJI4zCx",
        "outputId": "27b9a35e-00b8-4a67-d8cb-c8cbd06214f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmSXEgjM41qi",
        "outputId": "32dc8a34-ff4b-46af-f533-2a0a9fe83d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzF-IGZI5Ygl",
        "outputId": "0067b0ad-292c-4d51-8cbd-3e3e3dd04f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Finetuning_NER_DLT_Assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5j-tow35qTR",
        "outputId": "aaffc702-c361-456d-844c-a2c3249ac8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Finetuning_NER_DLT_Assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7zrC9KS5zkG",
        "outputId": "d5ca12c0-6b2d-4354-eb9e-6a47c43886c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finetuning_ner.ipynb  train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "X3VHE2536AlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "import torch\n",
        "import copy\n",
        "import logging\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Dict, Tuple, Optional, Union"
      ],
      "metadata": {
        "id": "T4n7-eGb6Uvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def span_to_label(labeled_spans: Dict[Tuple[int, int], str], tokens: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convert entity spans to labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labeled_spans: labeled span dictionary: {(start, end): label}\n",
        "    tokens: a list of tokens, used to check if the spans are valid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    a list of string labels\n",
        "    \"\"\"\n",
        "    if labeled_spans:\n",
        "        assert list(labeled_spans.keys())[-1][1] <= len(tokens), ValueError(\"label spans out of scope!\")\n",
        "\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "    for (start, end), label in labeled_spans.items():\n",
        "        if type(label) == list or type(label) == tuple:\n",
        "            lb = label[0][0]\n",
        "        else:\n",
        "            lb = label\n",
        "        labels[start] = \"B-\" + lb\n",
        "        if end - start > 1:\n",
        "            labels[start + 1 : end] = [\"I-\" + lb] * (end - start - 1)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "mv1CqOiA6SIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def span_list_to_dict(span_list: List[list]) -> Dict[Tuple[int, int], Union[str, tuple]]:\n",
        "    \"\"\"\n",
        "    convert entity label span list to span dictionaries\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    span_list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    span_dict\n",
        "    \"\"\"\n",
        "    span_dict = dict()\n",
        "    for span in span_list:\n",
        "        span_dict[(span[0], span[1])] = span[2]\n",
        "    return span_dict"
      ],
      "metadata": {
        "id": "_Vs_Xd8F6ItJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_from_json(file_dir: str):\n",
        "    \"\"\"\n",
        "    Load data stored in the current data format.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_dir: str\n",
        "        file directory\n",
        "\n",
        "    \"\"\"\n",
        "    with open(file_dir, \"r\", encoding=\"utf-8\") as f:\n",
        "        data_list = json.load(f)\n",
        "\n",
        "    tk_seqs = list()\n",
        "    lbs_list = list()\n",
        "\n",
        "    for inst in data_list:\n",
        "        # get tokens\n",
        "        tk_seqs.append(inst[\"text\"])\n",
        "\n",
        "        # get true labels\n",
        "        lbs = span_to_label(span_list_to_dict(inst[\"label\"]), inst[\"text\"])\n",
        "        lbs_list.append(lbs)\n",
        "\n",
        "    return tk_seqs, lbs_list"
      ],
      "metadata": {
        "id": "-nFjm0JX50UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5VOag6XW6AAQ",
        "outputId": "167749c2-178c-4249-8c30-996b4fcdf699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Finetuning_NER_DLT_Assignment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text,labels=load_data_from_json(\"/content/drive/MyDrive/Finetuning_NER_DLT_Assignment/train.json\")"
      ],
      "metadata": {
        "id": "w5Uvg3lC6dNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text),len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPQVxnrK6fvV",
        "outputId": "cc9399da-fdd2-4562-de0d-29a9a1295f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[1],labels[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4wVKnq6rBs",
        "outputId": "956387a6-8a9b-4ec8-9ab4-0f2c89cf6c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['--', 'Reuter', 'London', 'Newsroom', '+44', '171', '542', '7658'],\n",
              " ['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt3ll7os7Ctf",
        "outputId": "bce3014a-bf28-430b-c44f-d9ead5b81d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils import send_example_telemetry\n",
        "\n",
        "send_example_telemetry(\"token_classification_notebook\", framework=\"pytorch\")"
      ],
      "metadata": {
        "id": "pbB4gW9H--pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "mabO9wiW_BHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "qAjx9Fwm_CrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "4VF_wt-y_EWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtMVZtoV_FzT",
        "outputId": "f647b660-e715-4889-cbda-119d166348c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyOAk_fN_Huj",
        "outputId": "69c36cec-f13d-466a-aee5-eada5206debb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[\"train\"].features['ner_tags']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PDPp4xj_JRX",
        "outputId": "59ec69de-d556-4efc-97f5-059d4979b32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pack_instances(**kwargs) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Convert attribute lists to a list of data instances, each is a dict with attribute names as keys\n",
        "    and one datapoint attribute values as values\n",
        "    \"\"\"\n",
        "\n",
        "    instance_list = list()\n",
        "    keys = tuple(kwargs.keys())\n",
        "    # print(keys)\n",
        "\n",
        "    for inst_attrs in zip(*tuple(kwargs.values())):\n",
        "        inst = dict(zip(keys, inst_attrs))\n",
        "        instance_list.append(inst)\n",
        "\n",
        "    return instance_list"
      ],
      "metadata": {
        "id": "LnopYUVE_MNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack_instances(instance_list: list[dict], attr_names: Optional[list[str]] = None):\n",
        "    \"\"\"\n",
        "    Convert a list of dict-type instances to a list of value lists,\n",
        "    each contains all values within a batch of each attribute\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    instance_list: list[dict]\n",
        "        a list of attributes\n",
        "    attr_names: list[str], optional\n",
        "        the name of the needed attributes. Notice that this variable should be specified\n",
        "        for Python versions that does not natively support ordered dict\n",
        "    \"\"\"\n",
        "    if not attr_names:\n",
        "        attr_names = list(instance_list[0].keys())\n",
        "    attribute_tuple = [[inst[name] for inst in instance_list] for name in attr_names]\n",
        "\n",
        "    return attribute_tuple"
      ],
      "metadata": {
        "id": "6S9vqJaRjzwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "FAZmFGd1AJjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", add_prefix_space=True)"
      ],
      "metadata": {
        "id": "TIxgHAnxAqHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenizer(text, add_special_tokens=True, is_split_into_words=True)"
      ],
      "metadata": {
        "id": "_CGY3XpaCN5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer(text[2], add_special_tokens=True, is_split_into_words=True)['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cPpttiQSCRh1",
        "outputId": "4bbd4bc1-d5ad-4449-abc0-2f6a7a1900e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] bugno tested positive for the banned hormone after the fifth stage of the tour, in which he finished third overall. [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb2idx={'O':1,'B-PER':1,'I-PER':2,'B-ORG':3,'I-ORG':4,'B-LOC':5,'I-LOC':6,'B-MISC':7,\"I-MISC\":8}"
      ],
      "metadata": {
        "id": "rAdl-epZW0hm"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_lbs_list = list()"
      ],
      "metadata": {
        "id": "OFmrlEBJVYG5"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in enumerate(tokenized_text['input_ids']):\n",
        "    word_ids=(tokenized_text.word_ids(batch_index=i))\n",
        "    prev_word_id=None\n",
        "    bert_lbs_list_i=[]\n",
        "    for word_id in word_ids:\n",
        "      if word_id is None:\n",
        "        bert_lbs_list_i.append(-100)\n",
        "      elif word_id != prev_word_id:\n",
        "        bert_lbs_list_i.append(lb2idx[labels[i][word_id]])\n",
        "      else:\n",
        "        bert_lbs_list_i.append(-100)\n",
        "\n",
        "      prev_word_id=word_id\n",
        "    bert_lbs_list.append(bert_lbs_list_i)"
      ],
      "metadata": {
        "id": "htjfYn_iWhj4"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bert_lbs_list),len(tokenized_text['input_ids']),len(tokenized_text['attention_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gF0v5puXfMn",
        "outputId": "24fbf812-ec58-48bf-8080-932e586ff503"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1st input id's\")\n",
        "print(tokenized_text['input_ids'][0])\n",
        "print('-'*20)\n",
        "print(\"1st attention masks\")\n",
        "print(tokenized_text['attention_mask'][0])\n",
        "print('-'*20)\n",
        "print(\"1st data point labels\")\n",
        "print(bert_lbs_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3BmOrWx_Qo3",
        "outputId": "a11eaf30-caf2-4cb6-c0fe-d9f5ad402582"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st input id's\n",
            "[101, 13144, 7460, 2000, 2068, 1012, 102]\n",
            "--------------------\n",
            "1st attention masks\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------\n",
            "1st data point labels\n",
            "[-100, 1, 1, 1, 1, 1, -100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_instances = pack_instances(\n",
        "            bert_tk_ids=tokenized_text['input_ids'],\n",
        "            bert_attn_masks=tokenized_text['attention_mask'],\n",
        "            bert_lbs=bert_lbs_list\n",
        "        )"
      ],
      "metadata": {
        "id": "GHUthwxZ_xo1"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data_instances)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6by2DgXz_xrL",
        "outputId": "234a1afb-1a35-4d5b-eb48-f42eda649ea9"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert_tk_ids': [101, 13144, 7460, 2000, 2068, 1012, 102],\n",
              " 'bert_attn_masks': [1, 1, 1, 1, 1, 1, 1],\n",
              " 'bert_lbs': [-100, 1, 1, 1, 1, 1, -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_instances[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TNZahvhB4p5",
        "outputId": "f88da16a-db39-40b2-f565-ccbc94ba0fc8"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert_tk_ids': [101,\n",
              "  1011,\n",
              "  1011,\n",
              "  2128,\n",
              "  19901,\n",
              "  2414,\n",
              "  2739,\n",
              "  9954,\n",
              "  1009,\n",
              "  4008,\n",
              "  18225,\n",
              "  5139,\n",
              "  2475,\n",
              "  6146,\n",
              "  27814,\n",
              "  102],\n",
              " 'bert_attn_masks': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'bert_lbs': [-100,\n",
              "  1,\n",
              "  -100,\n",
              "  3,\n",
              "  -100,\n",
              "  4,\n",
              "  4,\n",
              "  -100,\n",
              "  1,\n",
              "  -100,\n",
              "  1,\n",
              "  1,\n",
              "  -100,\n",
              "  1,\n",
              "  -100,\n",
              "  -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxhg1a4lB46w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_fn(**kwargs):\n",
        "  print(tuple(kwargs.keys()))\n",
        "  print(len(tuple(kwargs.values())))"
      ],
      "metadata": {
        "id": "YmL3C8l-AcZQ"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_fn(bert_tk_ids=tokenized_text['input_ids'],\n",
        "            bert_attn_masks=tokenized_text['attention_mask'],\n",
        "            bert_lbs=bert_lbs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVH11dojAj8T",
        "outputId": "897c1b78-cb46-461c-850b-64b03d74e002"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('bert_tk_ids', 'bert_attn_masks', 'bert_lbs')\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tk_ids, attn_masks, lbs = unpack_instances(data_instances, [\"bert_tk_ids\", \"bert_attn_masks\", \"bert_lbs\"])"
      ],
      "metadata": {
        "id": "XMxcZ3FyCYXe"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16"
      ],
      "metadata": {
        "id": "2UK9GUdbCYZz"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk_ids=tk_ids[:16]\n",
        "attn_masks=attn_masks[:16]\n",
        "lbs=lbs[:16]"
      ],
      "metadata": {
        "id": "xDb3PR8vO0K6"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_batch_length=max([len(i) for i in tk_ids])"
      ],
      "metadata": {
        "id": "_0rz1DC6PNVz"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk_ids = tokenizer.pad({'input_ids': tk_ids}, padding='longest', return_tensors=\"pt\")[\"input_ids\"].to(torch.int64)"
      ],
      "metadata": {
        "id": "Qf3EyKV-PFHU"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_masks = tokenizer.pad({'input_ids': attn_masks}, padding='longest', return_tensors=\"pt\")[\"input_ids\"].to(torch.int64)"
      ],
      "metadata": {
        "id": "cd-2TItoQQiq"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbs=torch.tensor([i+[collate_fn.label_pad_token_id]*(max_batch_length-len(i)) for i in lbs]).to(torch.int64)"
      ],
      "metadata": {
        "id": "mASuSzcFUkeo"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzr-kZkqVEv7",
        "outputId": "61be5c0f-237f-4ef9-bcda-de55838d5ade"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-100,    1, -100,    3, -100,    4,    4, -100,    1, -100,    1,    1,\n",
              "        -100,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn.label_pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrgCAREeNY84",
        "outputId": "2a65b38c-f1e1-4329-ee62-fcf32c729b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-100"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification"
      ],
      "metadata": {
        "id": "3NdcoAOyLhIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "d3cLjjpMLqgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn(data_instances[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "uBPIvaKoLzKe",
        "outputId": "4a467423-45d8-41c4-826d-7c53ce3311dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-c14d67862105>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_instances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"np\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mno_labels_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         batch = self.tokenizer.pad(\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mno_labels_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;31m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3223\u001b[0m                 \u001b[0;34m\"You should supply an encoding or a list of encodings to this method \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m                 \u001b[0;34mf\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['bert_tk_ids', 'bert_attn_masks', 'bert_lbs']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pack_instances(**kwargs) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Convert attribute lists to a list of data instances, each is a dict with attribute names as keys\n",
        "    and one datapoint attribute values as values\n",
        "    \"\"\"\n",
        "\n",
        "    instance_list = list()\n",
        "    keys = tuple(kwargs.keys())\n",
        "    # print(keys)\n",
        "\n",
        "    for inst_attrs in zip(*tuple(kwargs.values())):\n",
        "        inst = dict(zip(keys, inst_attrs))\n",
        "        instance_list.append(inst)\n",
        "\n",
        "    return instance_list"
      ],
      "metadata": {
        "id": "pS1y0wFJ_xwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1g8CKIv2AbDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dict={'input_ids': [[101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],[101,10100,102]],\n",
        " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0,0,0]],\n",
        " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],[1,1,1]]}\n",
        "\n",
        "tk_ids, attn_masks, lbs = unpack_instances(pack_instances(**input_dict),attr_names=['input_ids','token_type_ids','attention_mask'])\n",
        "\n",
        "# Padding using the tokenizer's pad method\n",
        "padded= tokenizer.pad([{\"input_ids\": tk_ids}],\n",
        "                              padding=\"longest\",\n",
        "                              return_tensors=\"pt\")\n",
        "\n",
        "# Getting the padded sequences\n",
        "tk_ids = padded[\"input_ids\"].to(torch.int64)\n",
        "# attn_masks = padded[\"attention_mask\"].to(torch.int64)\n",
        "# lbs = padded[\"labels\"].to(torch.int64)"
      ],
      "metadata": {
        "id": "eAbtE0IqaWTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}